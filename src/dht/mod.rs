//! A Kademlia Distributed Hash Table (DHT) implementation.
//!
//! This module provides the core functionality for a peer-to-peer represents
//! a node in te network with routing, storage, and communication capabilities.

pub mod config;
pub mod peer;
pub mod rpc;

mod connection;
mod kbucket;
mod lookup;
mod metrics;
mod node;
mod replication;
mod storage;

use anyhow::{Context, Result};
use dashmap::DashMap;
use tokio::{
    io::{AsyncReadExt, AsyncWriteExt},
    time::timeout,
};

use std::{
    net::SocketAddr,
    sync::{Arc, atomic::Ordering},
};

use crate::{
    dht::{
        config::DhtConfig,
        connection::ConnectionPool,
        kbucket::KBucket,
        metrics::{
            DhtMetrics, DhtStats,
            utils::{record_find_attempt, record_store_attempt},
        },
        node::NodeId,
        peer::PeerInfo,
        rpc::DhtRpc,
        storage::{
            StoredValue, create_stored_value, deserialize_value, find_in_local_storage,
            serialize_value,
        },
    },
    helpers::now,
};

/// A node in the Kademlia DHT network.
///
/// Each node maintains:
/// - A routing table of known peers
/// - A key-value storage
/// - Connection pooling for efficient communication
///
/// # Examples
///
/// ```no_run
/// use rust_p2p_node::dht::{DhtNode, config::DhtConfig};
/// use std::net::SocketAddr;
///
/// #[tokio::main]
/// async fn main() {
///     let addr: SocketAddr = "127.0.0.1:8080".parse().unwrap();
///     let node = DhtNode::new(addr, Some(DhtConfig::default()));
///
///     // Store a value
///     node.store(b"key".to_vec(), b"value".to_vec()).await.unwrap();
///
///     // Retrieve a value
///     let value = node.find_value(b"key".to_vec()).await;
///     assert_eq!(value, Some(b"value".to_vec()));
/// }
/// ```
#[derive(Clone)]
pub struct DhtNode {
    /// This node's identifier
    pub id: NodeId,
    /// This node's network address
    pub addr: SocketAddr,
    /// Kademlia routing table (organized as 256 k-buckets)
    pub routing_table: Arc<DashMap<u8, KBucket>>,
    /// Distibuted key-value storage
    pub storage: Arc<DashMap<Vec<u8>, Vec<u8>>>,
    /// Pool of TCP connections to other nodes
    pub connection_pool: ConnectionPool,
    pub config: DhtConfig,
    pub metrics: Arc<DhtMetrics>,
}

impl DhtNode {
    /// Creates a new DHT node with the specified address.
    ///
    /// The node's ID is generated by hashing its address.
    pub fn new(addr: SocketAddr, config: Option<DhtConfig>) -> Self {
        let id = NodeId::new(addr.to_string().as_bytes());
        let config = config.unwrap_or_default();

        Self {
            id,
            addr,
            routing_table: Self::create_routing_table(),
            storage: Self::create_storage(config.storage.max_entries),
            connection_pool: ConnectionPool::new(
                config.connection_pool.max_connections_per_peer,
                config.connection_pool.max_idle_time,
            ),
            config,
            metrics: DhtMetrics::new(),
        }
    }

    /// Adds a peer to the routing table.
    ///
    /// The peer is placed in the appropriate k-bucket based on its distance
    /// from this node. If the bucket is full, the peer may not be added.
    pub fn add_peer(&self, peer: PeerInfo) {
        let distance = self.id.distance(&peer.id);
        let bucket_index = self.get_bucket_index(&distance);

        self.routing_table
            .entry(bucket_index)
            .or_insert_with(|| KBucket {
                peers: Vec::new(),
                max_size: 20,
            })
            .get_peers()
            .retain(|p| p.id != peer.id);

        if let Some(mut bucket) = self.routing_table.get_mut(&bucket_index) {
            if bucket.peers.len() < bucket.max_size {
                bucket.peers.push(peer);
            }
        }
    }

    /// Updates the last seen timestamp for a peer.
    ///
    /// This helps maintain fresh routing information by marking active peers.
    pub fn update_peer_last_seen(&self, peer_id: &NodeId) {
        let distance = self.id.distance(&peer_id);
        let bucket_index = self.get_bucket_index(&distance);

        if let Some(mut bucket) = self.routing_table.get_mut(&bucket_index) {
            if let Some(peer) = bucket.peers.iter_mut().find(|p| &p.id == peer_id) {
                peer.last_seen = now();
            }
        }
    }

    /// Removes peers that haven't been seen within the specified duration.
    ///
    /// This helps maintain an up-to-date routing table by removing stale entries.
    pub fn remove_inactive_peers(&self, inactive_duration: u64) {
        let now = now();

        for mut bucket in self.routing_table.iter_mut() {
            bucket
                .value_mut()
                .peers
                .retain(|peer| now.saturating_sub(peer.last_seen) < inactive_duration);
        }
    }

    /// Stores a key-value pair in the DHT.
    ///
    /// The value is stored locally and replicated on the k closest nodes.
    pub async fn store(&self, key: Vec<u8>, value: Vec<u8>) -> Result<()> {
        let stored = create_stored_value(
            value,
            self.addr,
            false,
            Some(self.config.storage.default_ttl),
        );
        let serialized = serialize_value(stored)?;

        let closest_peers = self.find_closest_peers_by_key(&key);

        self.metrics.set_known_peers(closest_peers.len() as u64);
        self.storage.insert(key.clone(), serialized.clone());

        let successes = self
            .replicate_to_peers_store(key.clone(), serialized.clone(), closest_peers)
            .await;

        record_store_attempt(&self.metrics, successes > 0);

        Ok(())
    }

    /// Looks up a value by key in the DHT
    ///
    /// Checks local storage first, then queries the k closest nodes if not found.
    pub async fn find_value(&self, key: Vec<u8>) -> Option<Vec<u8>> {
        let mut found_values = vec![];

        find_in_local_storage(&self, &mut found_values, key.clone());

        let closest_peers = self.find_closest_peers_by_key(&key);

        self.metrics.set_known_peers(closest_peers.len() as u64);

        let successes = self
            .replicate_to_peers_find(&mut found_values, key.clone(), closest_peers)
            .await;

        record_find_attempt(&self.metrics, successes > 0);

        self.resolve_conflict(found_values).map(|v| v.data)
    }

    /// Handles incoming RPC messages.
    ///
    /// This is the main request processing entry poing for the DHT node.
    pub async fn handle_rpc(&self, rpc: DhtRpc) -> DhtRpc {
        match rpc {
            DhtRpc::Ping => DhtRpc::Pong,
            DhtRpc::FindNode(target) => {
                let peers = self.find_closest_peers(&target, 8);
                DhtRpc::FindNodeResponse(peers)
            }
            DhtRpc::FindValue(key) => {
                let value = self.storage.get(&key).map(|v| v.clone());
                DhtRpc::FindValueResponse(value)
            }
            DhtRpc::Store(key, value) => {
                // self.store(key, value).await.unwrap();
                self.storage.insert(key, value);
                DhtRpc::Pong
            }
            _ => DhtRpc::Pong,
        }
    }

    /// Sends an RPC message to another node and returns the response.
    ///
    /// This handles connection management and message serialization.
    pub async fn send_rpc(&self, peer: SocketAddr, message: DhtRpc) -> Result<DhtRpc> {
        let mut conn = self.connection_pool.get_connection(peer).await?;

        let serialized = bincode::serialize(&message)?;
        let len = (serialized.len() as u32).to_be_bytes();

        conn.write_all(&len)
            .await
            .context("Failed to send message length")?;

        conn.write_all(&serialized)
            .await
            .context("Failed to send message")?;

        let mut len_buf = [0u8; 4];
        conn.read_exact(&mut len_buf)
            .await
            .context("Failed to read response length")?;

        let len = u32::from_be_bytes(len_buf) as usize;
        let mut response_buf = vec![0u8; len];
        conn.read_exact(&mut response_buf)
            .await
            .context("Failed to read response")?;

        let response = bincode::deserialize(&response_buf)?;
        Ok(response)
    }

    /// Connects to known peers to join the DHT network
    pub async fn bootstrap(&self, known_peers: Vec<SocketAddr>) -> Result<()> {
        for peer in known_peers {
            match self.send_rpc(peer, DhtRpc::FindNode(self.id.clone())).await {
                Ok(DhtRpc::FindNodeResponse(peers)) => {
                    for peer_info in peers {
                        self.add_peer(peer_info);
                    }
                }
                _ => continue,
            }
        }

        Ok(())
    }

    /// Returns a photo DHT stats
    pub fn get_stats(&self) -> DhtStats {
        DhtStats {
            store_ops: self.metrics.store_ops.load(Ordering::Relaxed),
            store_success: self.metrics.store_success.load(Ordering::Relaxed),
            find_value_ops: self.metrics.find_value_ops.load(Ordering::Relaxed),
            find_value_success: self.metrics.find_value_success.load(Ordering::Relaxed),
            rpc_requests: self.metrics.rpc_requests.load(Ordering::Relaxed),
            rpc_failures: self.metrics.rpc_failures.load(Ordering::Relaxed),
            known_peers: self.metrics.known_peers.load(Ordering::Relaxed),
            storage_size: self.storage.len() as u64,
        }
    }

    pub async fn start_maintenance_service(&self) {
        let node = self.clone();
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(node.config.maintenance_interval);

            loop {
                interval.tick().await;

                node.check_peers_health().await;

                node.clean_expired().await;

                node.check_data_availability().await;
            }
        });
    }

    /// Calculates the k-bucket index for a given distance.
    ///
    /// This implements the Kademlia routing table structure where each bucket
    /// holds nodes at specific distance ranges.
    fn get_bucket_index(&self, distance: &[u8; 32]) -> u8 {
        for i in 0..32 {
            for j in (0..8).rev() {
                if (distance[i] >> j) & 1 == 1 {
                    return (i * 8 + (7 - j)) as u8;
                }
            }
        }
        255
    }

    fn create_routing_table() -> Arc<DashMap<u8, KBucket>> {
        Arc::new(DashMap::with_capacity(256))
    }

    fn create_storage(max_entries: usize) -> Arc<DashMap<Vec<u8>, Vec<u8>>> {
        Arc::new(DashMap::with_capacity(max_entries))
    }

    /// Finds the k closest peers to a given key according to the XOR metric.
    ///
    /// This is a core Kademlia operation used for routing and value lookup.
    fn find_closest_peers(&self, key: &NodeId, k: usize) -> Vec<PeerInfo> {
        let rt = &self.routing_table;
        let mut all_peers = Vec::new();

        for bucket in rt.iter() {
            all_peers.extend(bucket.value().peers.iter().cloned());
        }

        all_peers.sort_by(|a, b| {
            let dist_a = key.distance(&a.id);
            let dist_b = key.distance(&b.id);
            dist_a.cmp(&dist_b)
        });

        all_peers.into_iter().take(k).collect()
    }

    fn find_closest_peers_by_key(&self, key: &Vec<u8>) -> Vec<PeerInfo> {
        let key_id = NodeId::new(&key);
        let replication_factor = self.config.replication.factor;
        self.find_closest_peers(&key_id, replication_factor)
    }

    fn resolve_conflict(&self, values: Vec<StoredValue>) -> Option<StoredValue> {
        values.into_iter().max_by_key(|v| v.version)
    }

    async fn repair_replication(&self, key: &Vec<u8>, value: Vec<u8>) {
        let current_value = match self.storage.get(key) {
            Some(v) => v.clone(),
            None => return,
        };

        if let Ok(stored_value) = deserialize_value(&current_value) {
            if stored_value.is_replica {
                return;
            }
        }

        let key_id = NodeId::new(key);
        let replication_factor = self.config.replication.factor * 2;
        let closest_peers = self.find_closest_peers(&key_id, replication_factor);

        for peer in closest_peers {
            if let Ok(DhtRpc::FindValueResponse(None)) = self
                .send_rpc(peer.addr, DhtRpc::FindValue(key.clone()))
                .await
            {
                let _ = self
                    .send_rpc(peer.addr, DhtRpc::Store(key.clone(), value.clone()))
                    .await;
            }
        }
    }

    async fn store_with_fallback(
        &self,
        key: Vec<u8>,
        value: Vec<u8>,
        original_nodes: Vec<SocketAddr>,
    ) -> Result<()> {
        let key_id = NodeId::new(&key);
        let replication_factor = self.config.replication.factor * 2;
        let closest_peers = self.find_closest_peers(&key_id, replication_factor);

        let mut success_count = 0;
        for peer in closest_peers {
            if success_count >= self.config.replication.factor {
                break;
            }

            let value_to_store = if original_nodes.contains(&peer.addr) {
                StoredValue {
                    data: value.clone(),
                    version: now(),
                    last_node: self.addr,
                    is_replica: false,
                    expiration: Some(now() + self.config.storage.default_ttl),
                    original_nodes: original_nodes.clone(),
                }
            } else {
                StoredValue {
                    data: value.clone(),
                    version: now(),
                    last_node: self.addr,
                    is_replica: true,
                    expiration: Some(now() + self.config.storage.default_ttl),
                    original_nodes: original_nodes.clone(),
                }
            };

            match self
                .send_rpc(
                    peer.addr,
                    DhtRpc::Store(key.clone(), serialize_value(value_to_store)?),
                )
                .await
            {
                Ok(_) => success_count += 1,
                Err(_) => continue,
            }
        }

        if success_count > 0 {
            Ok(())
        } else {
            Err(anyhow::anyhow!("Failed to store on any peer"))
        }
    }

    async fn check_peers_health(&self) {
        let mut dead_peers = Vec::new();

        for bucket in self.routing_table.iter() {
            for peer in bucket.value().peers.iter() {
                match timeout(
                    self.config.operation_timeout,
                    self.send_rpc(peer.addr, DhtRpc::Ping),
                )
                .await
                {
                    Ok(Ok(DhtRpc::Pong)) => {
                        self.update_peer_last_seen(&peer.id);
                    }
                    _ => {
                        dead_peers.push(peer.clone());
                    }
                }
            }
        }

        for peer in dead_peers {
            self.handle_dead_peer(&peer).await;
        }
    }

    async fn handle_dead_peer(&self, peer: &PeerInfo) {
        let distance = self.id.distance(&peer.id);
        let bucket_index = self.get_bucket_index(&distance);

        if let Some(mut bucket) = self.routing_table.get_mut(&bucket_index) {
            bucket.peers.retain(|p| p.id != peer.id);
        }

        let mut to_replicate = Vec::new();

        for entry in self.storage.iter() {
            if let Ok(value) = deserialize_value(entry.value()) {
                if value.original_nodes.contains(&peer.addr) {
                    to_replicate.push((entry.key().clone(), value.data));
                }
            }
        }

        for (key, value) in to_replicate {
            let _ = self.store_with_fallback(key, value, vec![peer.addr]).await;
        }
    }

    async fn check_data_availability(&self) {
        for entry in self.storage.iter() {
            if let Ok(value) = deserialize_value(entry.value()) {
                if !value.is_replica {
                    let key = entry.key();
                    let existing_replicas = self.count_existing_replicas(key).await;

                    if existing_replicas < self.config.replication.factor {
                        self.repair_replication(key, value.data).await;
                    }
                }
            }
        }
    }

    async fn count_existing_replicas(&self, key: &Vec<u8>) -> usize {
        let key_id = NodeId::new(key);
        let closest_peers = self.find_closest_peers(&key_id, self.config.replication.factor * 2);

        let mut count = 0;
        for peer in closest_peers {
            if let Ok(DhtRpc::FindValueResponse(Some(_))) = self
                .send_rpc(peer.addr, DhtRpc::FindValue(key.clone()))
                .await
            {
                count += 1;
            }
        }

        count
    }

    async fn clean_expired(&self) {
        let current_time = now();
        self.storage.retain(|_, value| {
            deserialize_value(value)
                .map(|v| v.is_valid(current_time))
                .unwrap_or(false)
        });
    }
}

#[cfg(test)]
mod dht_node_tests {
    use std::net::{IpAddr, Ipv4Addr, SocketAddr};

    use crate::{
        dht::{NodeId, PeerInfo},
        helpers::create_test_node,
    };

    #[tokio::test]
    async fn test_store_and_find_value() {
        let node = create_test_node(8090);

        let key = b"test_key".to_vec();
        let value = b"test_value".to_vec();

        node.store(key.clone(), value.clone()).await.unwrap();

        let found = node.find_value(key).await;
        assert_eq!(found, Some(value));
    }

    #[tokio::test]
    async fn test_find_closes_peers() {
        let node = create_test_node(8090);

        for i in 0..10 {
            let peer = PeerInfo {
                id: NodeId::new(&[i; 32]),
                addr: SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 8000 + i as u16),
                last_seen: 0,
            };
            node.add_peer(peer);
        }

        let target = NodeId::new(b"target");
        let closest = node.find_closest_peers(&target, 3);

        assert_eq!(closest.len(), 3);

        for peer in &closest {
            let dist = target.distance(&peer.id);
            assert!(dist.iter().any(|&b| b != 0));
        }
    }

    #[tokio::test]
    async fn test_handle_rpc() {
        use crate::dht::DhtRpc;

        let node = create_test_node(8090);

        match node.handle_rpc(DhtRpc::Ping).await {
            DhtRpc::Pong => (),
            _ => panic!("Expected Pong"),
        };

        let target = NodeId::new(b"target");
        match node.handle_rpc(DhtRpc::FindNode(target.clone())).await {
            DhtRpc::FindNodeResponse(_) => (),
            _ => panic!("Expected FindNodeResponse"),
        };

        let key = b"key".to_vec();
        let value = b"value".to_vec();
        match node
            .handle_rpc(DhtRpc::Store(key.clone(), value.clone()))
            .await
        {
            DhtRpc::Pong => (),
            _ => panic!("Expected Pong"),
        };

        assert!(node.storage.contains_key(&key));
    }

    // #[tokio::test]
    // async fn test_stored_value_validation() {
    //     let node = create_test_node(8090);
    //     let key = b"test_key".to_vec();

    //     // Значение с истёкшим TTL
    //     let expired_value = StoredValue {
    //         data: b"expired".to_vec(),
    //         version: 1,
    //         last_node: node.addr,
    //         is_replica: false,
    //         expiration: Some(now() - 10), // Уже истёк
    //         original_nodes: vec![node.addr],
    //     };

    //     node.storage
    //         .insert(key.clone(), serialize_value(expired_value).unwrap().clone());
    //     assert_eq!(node.find_value(key.clone()).await, None);

    //     // Значение без TTL
    //     let persistent_value = StoredValue {
    //         data: b"persistent".to_vec(),
    //         version: 2,
    //         last_node: node.addr,
    //         is_replica: false,
    //         expiration: None, // Никогда не истекает
    //         original_nodes: vec![node.addr],
    //     };

    //     node.storage
    //         .insert(key.clone(), serialize_value(persistent_value).unwrap());
    //     assert_eq!(
    //         node.find_value(key.clone()).await,
    //         Some(b"persistent".to_vec())
    //     );
    // }
}
